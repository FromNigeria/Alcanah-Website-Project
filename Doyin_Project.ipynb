{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1EERSlBpWymRwk4ojOIR8VT9Xpfgoj4-H",
      "authorship_tag": "ABX9TyMuorqbLQAYteb4bLWe0LQN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FromNigeria/Alcanah-Website-Project/blob/master/Doyin_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFHssw0SyoV6"
      },
      "outputs": [],
      "source": [
        "#pip install vaderSentiment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Solution\n",
        "# import our modules in order to extract product details for a clientâ€™s pre-defined list of ASINs\n",
        "import pandas as pd\n",
        "import requests\n",
        "import re\n",
        "import string\n",
        "import numpy as np \n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "from wordcloud import WordCloud\n",
        "from textblob import TextBlob\n",
        "pd.set_option('display.max_colwidth', None)#None\n",
        "import unicodedata\n",
        "import warnings\n",
        "from tkinter import X\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer as tfidf\n",
        "from nltk.corpus import stopwords\n",
        "import warnings\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.naive_bayes import MultinomialNB,BernoulliNB\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import KFold\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as sid\n",
        "import nltk\n",
        "nltk.download(\"popular\")\n"
      ],
      "metadata": {
        "id": "5VKNSlzxzJ8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "id": "Wx-npIjyzJ_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking column datatype using lambda\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/subset_tweets.csv\",parse_dates=['Date'], dayfirst=True,usecols=['Date','text'])\n",
        "print(\"No of Rows and Columns\",df.shape)\n",
        "raw_data = df.copy()\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "HB32D2Qw51e1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data cleaning and Preprocessing"
      ],
      "metadata": {
        "id": "naKsJKJZ51ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['text']= df['text'].str.lower()#lowercase the text\n",
        "\n",
        "def strip_links(text):##remove hyperlinks url\n",
        "    link_regex    = re.compile('((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)', re.DOTALL)\n",
        "    links         = re.findall(link_regex, text)\n",
        "    for link in links:\n",
        "        text = text.replace(link[0], ', ')    \n",
        "    return text\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: strip_links(x))  \n",
        "\n",
        "#2ndly strip_all_entities like tweeter mentions, hashtags and stuff\n",
        "\n",
        "def strip_all_entities(text):\n",
        "    entity_prefixes = ['@','#']\n",
        "    for separator in  string.punctuation:\n",
        "        if separator not in entity_prefixes :\n",
        "            text = text.replace(separator,' ')\n",
        "    words = []\n",
        "    for word in text.split():\n",
        "        word = word.strip()\n",
        "        if word:\n",
        "            if word[0] not in entity_prefixes:\n",
        "                words.append(word)\n",
        "    return ' '.join(words)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: strip_all_entities(x))\n",
        "\n",
        "df['text'] = df['text'].str.findall('\\w{4,}').str.join(' ')\n",
        "\n",
        "\n",
        "#3 get rid of special_characters\n",
        "def special_characters(text):\n",
        "    return re.sub(r\"[^A-Za-z0-9\\s]+\", \"\", text)\n",
        "df['text'] = df['text'].apply(lambda x: special_characters(x))\n",
        "\n",
        "\n",
        "#remove some special nos too\n",
        "def special_nos(text):\n",
        "    return re.sub(r\"\\b[0-9]+\\b\\s*\", \"\", text)\n",
        "df['text'] = df['text'].apply(lambda x: special_nos(x))\n",
        "\n",
        "#remove duplicates words\n",
        "df['text'] = df['text'].str.replace(r'\\b(\\w+)(\\s+\\1)+\\b', r'\\1')\n",
        "\n",
        "\n",
        "#Removing '\\n' in text\n",
        "df['text'] = df['text'].replace(r'\\s+|\\\\n', ' ', regex=True) \n",
        "\n",
        "# we can still try remove numbers from string like e.g donald65 -->  donald\n",
        "df['text'] = df['text'].str.replace('\\d+', '')\n",
        "\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "iKqFNk4751kE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "id": "UPzkLS2AQnu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#stop_words = 'twitter','cryptosoul', 'retweets', 'bicoin','coinmarketcap','bdt','ref', 'doge','idap','ppbb','kyc', 'cryptocurrency','binance','ethereum', 'tweets', 'tweet', 'fscebook', 'btc', 'crypto', 'utc','xrp', 'like', 'currencies', 'coin','litecoin', 'altcoin','btcusd','altcoin','airdrop'\n",
        "#df_clean['text'] = df_clean['text'].replace(stop_words,'', inplace = True)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('bitcoin', '')\n",
        "                                if isinstance(x, str) else x).astype(str)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('airdrop', '')\n",
        "                                if isinstance(x, str) else x).astype(str)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('cryptosoul', '')\n",
        "                                if isinstance(x, str) else x).astype(str)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('satoshi', '')\n",
        "                                if isinstance(x, str) else x).astype(str)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('utc', '')\n",
        "                                if isinstance(x, str) else x).astype(str)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('btc', '')\n",
        "                                if isinstance(x, str) else x).astype(str)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('bitsler', '')\n",
        "                                if isinstance(x, str) else x).astype(str)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('mgkdve', '')\n",
        "                                if isinstance(x, str) else x).astype(str)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('cryptosoul', '')\n",
        "                                if isinstance(x, str) else x).astype(str)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('trading', '')\n",
        "                                if isinstance(x, str) else x).astype(str)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('bitsler', '')\n",
        "                                if isinstance(x, str) else x).astype(str)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('coin', '')\n",
        "                                if isinstance(x, str) else x).astype(str)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('bdt', '')\n",
        "                                if isinstance(x, str) else x).astype(str)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('kyc', '')\n",
        "                                if isinstance(x, str) else x).astype(str)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('amp', '')\n",
        "                                if isinstance(x, str) else x).astype(str)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('bitdepositary', '')\n",
        "                                if isinstance(x, str) else x).astype(str)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('blockchain', '')\n",
        "                                if isinstance(x, str) else x).astype(str)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('p2pb2b', '')\n",
        "                                if isinstance(x, str) else x).astype(str)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('idap', '')\n",
        "                                if isinstance(x, str) else x).astype(str)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('ref', '')\n",
        "                                if isinstance(x, str) else x).astype(str)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('crypto', '')\n",
        "                                if isinstance(x, str) else x).astype(str)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('trading', '')\n",
        "                                if isinstance(x, str) else x).astype(str)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('cryptocurrencies', '')\n",
        "                                if isinstance(x, str) else x).astype(str)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('lablxvx', '')\n",
        "                                if isinstance(x, str) else x).astype(str)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('lxvx', '')\n",
        "                                if isinstance(x, str) else x).astype(str)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('tweet', '')\n",
        "                                if isinstance(x, str) else x).astype(str)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('twitter', '')\n",
        "                                if isinstance(x, str) else x).astype(str)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('binance', '')\n",
        "                              if isinstance(x, str) else x).astype(str)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('coinmarket', '')\n",
        "                                if isinstance(x, str) else x).astype(str)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('ethereum', '')\n",
        "                                if isinstance(x, str) else x).astype(str)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('ada', '')\n",
        "                                if isinstance(x, str) else x).astype(str)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('follow', '')\n",
        "                                if isinstance(x, str) else x).astype(str)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('retweet', '')\n",
        "                                if isinstance(x, str) else x).astype(str)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('just', '')\n",
        "                                if isinstance(x, str) else x).astype(str)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('xrp', '')\n",
        "                                if isinstance(x, str) else x).astype(str)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('eth', '')\n",
        "                                if isinstance(x, str) else x).astype(str)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('utc', '')\n",
        "                                if isinstance(x, str) else x).astype(str)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('$', '')\n",
        "                                if isinstance(x, str) else x).astype(str)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('usd', '')\n",
        "                                if isinstance(x, str) else x).astype(str)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('exmo', '')\n",
        "                                if isinstance(x, str) else x).astype(str)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('think', '')\n",
        "                                if isinstance(x, str) else x).astype(str)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('good', '')\n",
        "                                if isinstance(x, str) else x).astype(str)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('ppbb', '')\n",
        "                                if isinstance(x, str) else x).astype(str)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('litecoin', '')\n",
        "                                if isinstance(x, str) else x).astype(str)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('altcoin', '')\n",
        "                                if isinstance(x, str) else x).astype(str)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('rt', '')\n",
        "                                if isinstance(x, str) else x).astype(str)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('btcusd', '')\n",
        "                                if isinstance(x, str) else x).astype(str)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('facebook', '')\n",
        "                                if isinstance(x, str) else x).astype(str)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('kyc', '')\n",
        "                                if isinstance(x, str) else x).astype(str)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('doge', '')\n",
        "                                if isinstance(x, str) else x).astype(str) \n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('woh', '')\n",
        "                                if isinstance(x, str) else x).astype(str)  \n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('erral', '')\n",
        "                                if isinstance(x, str) else x).astype(str) \n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('com', '')\n",
        "                                if isinstance(x, str) else x).astype(str)                                \n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('erral', '')\n",
        "                                if isinstance(x, str) else x).astype(str) \n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('like', '')\n",
        "                                if isinstance(x, str) else x).astype(str) \n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: x.replace('link', '')\n",
        "                                if isinstance(x, str) else x).astype(str)                              \n",
        "\n",
        "df_clean = df.copy()                             \n",
        "df_clean.tail()"
      ],
      "metadata": {
        "id": "eDIUC3kxF3zA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean.head()"
      ],
      "metadata": {
        "id": "CtZtMXpNF9hT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean['text'] = df_clean['text'].str.replace(r'\\b(\\w{1,3})\\b', '')#remove words less than 3 for me\n",
        "#then count each word occurrence\n",
        "cv = CountVectorizer(stop_words = 'english')\n",
        "words = cv.fit_transform(df_clean.text)\n",
        "\n",
        "sum_words = words.sum(axis=0)\n",
        "\n",
        "words_freq = [(word, sum_words[0, i]) for word, i in cv.vocabulary_.items()]\n",
        "words_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)\n",
        "words_freq"
      ],
      "metadata": {
        "id": "aRo0SruC8iib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#then i visualize the word frequency\n",
        "frequency = pd.DataFrame(words_freq, columns=['word', 'freq'])\n",
        "\n",
        "frequency.head(30).plot(x='word', y='freq', kind='bar', figsize=(15, 7), color = 'blue')\n",
        "plt.title(\"Most Frequently Occuring Words - Top 30\")"
      ],
      "metadata": {
        "id": "OHjo0Jg151mm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#word cloud\n",
        "from wordcloud import WordCloud\n",
        "tweets = df_clean['text'].tolist()#df_clean['text']\n",
        "\n",
        "tweets_as_one_string =\" \".join(tweets)\n",
        "tweets_as_one_string\n",
        "\n",
        "plt.figure(figsize=(8,7))#setting figure size\n",
        "plt.imshow(WordCloud().generate(tweets_as_one_string))\n"
      ],
      "metadata": {
        "id": "5rI_CQGA51o-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data['clean_tweets'] = df['text']#trans\n",
        "#df.head()\n",
        "raw_data.head()\n"
      ],
      "metadata": {
        "id": "g4OZn-fWw5rP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_vader = df_clean.copy()\n",
        "df_vader.head()"
      ],
      "metadata": {
        "id": "BuVMWhVQd6s6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#VARder Model\n",
        "#2.\tTo what extent do tweets predict the next day price direction of bitcoin?\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "cs = []\n",
        "for row in range(len(df_vader)):\n",
        "    cs.append(analyzer.polarity_scores(df_vader['text'].iloc[row])['compound'])\n",
        "\n",
        "df_vader['compound_vader_score'] = cs\n",
        "df_vader = df_vader[(df_vader[['compound_vader_score']] != 0).all(axis=1)].reset_index(drop=True)#create a new column compound\n",
        "\n",
        "df_vader.head()"
      ],
      "metadata": {
        "id": "aeizxgq-51q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#let's look at the extreme maximum and minimum compound scores for same day in sub_tweets comments\n",
        "\n",
        "unique_dates = df_vader['Date'].unique()#group by unique dates\n",
        "grouped_dates = df_vader.groupby(['Date'])\n",
        "keys_dates = list(grouped_dates.groups.keys())#get the key dates\n",
        "\n",
        "max_cs = []#cretae empty max\n",
        "min_cs = [] #min \n",
        "\n",
        "for key in grouped_dates.groups.keys():#using for loop\n",
        "    data = grouped_dates.get_group(key)\n",
        "    if data[\"compound_vader_score\"].max() > 0:\n",
        "        max_cs.append(data[\"compound_vader_score\"].max())\n",
        "    elif data[\"compound_vader_score\"].max() < 0:\n",
        "        max_cs.append(0)\n",
        "    \n",
        "    if data[\"compound_vader_score\"].min() < 0:\n",
        "        min_cs.append(data[\"compound_vader_score\"].min())\n",
        "    elif data[\"compound_vader_score\"].min() > 0:\n",
        "        min_cs.append(0)\n",
        "    \n",
        "extreme_scores_dict = {'Date':keys_dates,'max_scores':max_cs,'min_scores':min_cs}\n",
        "extreme_scores_df = pd.DataFrame(extreme_scores_dict)\n",
        "extreme_scores_df"
      ],
      "metadata": {
        "id": "I6II6ldH51uQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lets sum and calculate the final VADER scores\n",
        "final_scores = []\n",
        "for i in range(len(extreme_scores_df)):\n",
        "    final_scores.append(extreme_scores_df['max_scores'].values[i] + extreme_scores_df['min_scores'].values[i])\n",
        "\n",
        "extreme_scores_df['final_scores'] = final_scores\n",
        "\n",
        "extreme_scores_df.head()"
      ],
      "metadata": {
        "id": "P4PTsSU2lC2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Using final our compound VADER scores with threshold to generate trade calls\n",
        "#knowing how volatile behavior of bitcoin market eh these days, we can try use 0.20 as maybe threshold value for making bitcoin calls in our model based on\n",
        "#kind of tweet commments for a given day\n",
        "# VADER trade calls - with threshold\n",
        "#you can check their documentation sha here, https://github.com/cjhutto/vaderSentiment\n",
        "vader_Buy=[]\n",
        "vader_Sell=[]\n",
        "for i in range(len(extreme_scores_df)):\n",
        "    if extreme_scores_df['final_scores'].values[i] > 0.20:\n",
        "        print(\"Bitcoin prediction Call for {row} is Buy.\".format(row=extreme_scores_df['Date'].iloc[i].date()))\n",
        "        vader_Buy.append(extreme_scores_df['Date'].iloc[i].date())\n",
        "    elif extreme_scores_df['final_scores'].values[i] < 0.20:\n",
        "        print(\"Bitcoin prediction Call for {row} is Sell.\".format(row=extreme_scores_df['Date'].iloc[i].date()))\n",
        "        vader_Sell.append(extreme_scores_df['Date'].iloc[i].date())"
      ],
      "metadata": {
        "id": "6iHUw89-lC4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_vader = df_clean.copy()\n",
        "df_vader.head()"
      ],
      "metadata": {
        "id": "PCsBbqO8lC7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as sid\n",
        "#from langdetect import detect_langs\n",
        "# so i initialize sentiment classifier\n",
        "sid = sid()\n",
        "\n",
        "# getting sentiment scores from that data sha\n",
        "sentiment = df_vader['text'].apply(sid.polarity_scores)\n",
        "\n",
        "# and then iConvert sentiment series into dataframe (each sentiment value gets its own column)\n",
        "sentiment = pd.DataFrame(sentiment.tolist())\n",
        "\n",
        "# i then try to merge data and sentiment df into one\n",
        "df_vader = df_vader.merge(sentiment, how = 'left', left_index = True, right_index = True)\n",
        "\n",
        "# iactually wanted to delete sentiment as its info is already in data but sha\n",
        "#del sentiment\n",
        "df_vader.head()"
      ],
      "metadata": {
        "id": "o9Bo0PqCwhSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# so lemme create a small function to categorize compound sentiment score\n",
        "# 0.5 threshold recommended on VADER documentation\n",
        "# you should read the comments, read its compound score, and determine your own cuttoffs\n",
        "def categorize_sentiment(x):\n",
        "    if x >= 0.5:\n",
        "        return 'positive_comment'\n",
        "    elif 0.5 > x > -0.5:\n",
        "        return 'neutral_comment'\n",
        "    elif -0.5 >= x:\n",
        "        return 'negative_comment'\n",
        "# apply function categorize_sentiment to ['compound']\n",
        "df_vader['sentiment'] = df_vader['compound'].apply(categorize_sentiment)\n",
        "\n",
        "# then we can immediately convert ['sentiment'] to categorical data type though at once\n",
        "df_vader['sentiment'] = pd.Categorical(df_vader['sentiment'])\n",
        "\n",
        "#sentiment should be category\n",
        "print(df_vader.dtypes)\n",
        "print(df_vader['sentiment'].value_counts(),sep=\"\\n\\n\")#let's check  the cat count sha.\n",
        "#df_vader['sentiment'].unique())\n",
        "df_vader.head()"
      ],
      "metadata": {
        "id": "wOLcJqLFxQwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert ['sentiment'] categories to binary variables in new df\n",
        "binary_sentiment = df_vader['sentiment'].str.get_dummies()\n",
        "print(binary_sentiment.sum())# count of how many of each category were classified\n",
        "binary_sentiment.head()"
      ],
      "metadata": {
        "id": "HGG4XV9qxQ7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# merge binary_sentiment with df_vader \n",
        "df_vader = df_vader.merge(binary_sentiment, how = 'left', left_index = True, right_index = True)\n",
        "df_vader.head()"
      ],
      "metadata": {
        "id": "l7v3I9s0xQ_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removing redundant variables iDon't need\n",
        "del df_vader['neg']\n",
        "del df_vader['pos']\n",
        "del df_vader['neu']\n",
        "del df_vader['compound']\n",
        "del df_vader['text']\n",
        "del df_vader['sentiment']\n",
        "#del binary_sentiment\n",
        "df_vader.head()\n"
      ],
      "metadata": {
        "id": "1ssbhQu-6b49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_vader = df_vader.groupby(by = ['Date']).sum()\n",
        "print(type(df_vader))\n",
        "df_vader = df_vader.reset_index()#reset index of DataFrame to row numbers, moving index to columns.\n",
        "print(df_vader.shape)\n",
        "df_vader.head()"
      ],
      "metadata": {
        "id": "qSp_28EN6b72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading Bitcoin Price\n",
        "#checking column datatype using lambda\n",
        "bitcoin = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/bitcoin.csv\",parse_dates=['Date'], dayfirst=True)\n",
        "bitcoin['Year'] = bitcoin['Date'].dt.year #i want the year\n",
        "bitcoin['month'] = bitcoin['Date'].dt.month\n",
        "print(\"No of Rows and Columns\",bitcoin.shape)\n",
        "print(\"dtypes\",bitcoin.dtypes)\n",
        "print(\"unique\",bitcoin['Year'].unique())\n",
        "bitcoin = bitcoin.loc[bitcoin['Year'] == 2019] #keep only 2019\n",
        "bitcoin = bitcoin.loc[(bitcoin['month'] == 1) | (bitcoin['month'] == 2) | (bitcoin['month'] == 3) | (bitcoin['month'] == 4)]#keep only jan Feb mar and April\n",
        "\n",
        "print(\"unique months\",bitcoin['month'].unique())\n",
        "print(\"only unique year left\",bitcoin['Year'].unique())\n",
        "del bitcoin['Year'] #del off\n",
        "del bitcoin['month'] #del off\n",
        "bitcoin = bitcoin.reset_index()\n",
        "del bitcoin['index'] #del off\n",
        "bitcoin.head()"
      ],
      "metadata": {
        "id": "mQN97VBW6b-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('bitcoin rows',bitcoin.shape)\n",
        "bitcoin.head()"
      ],
      "metadata": {
        "id": "67qLDAq46cAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('tweets rows',df_vader.shape)\n",
        "df_vader.head(2)"
      ],
      "metadata": {
        "id": "aqLbA2Fh6cDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#just doing a bit of mental data manipulation here \n",
        "# creating new date variable that is the same name as the date variable in `bitcoin\n",
        "bitcoin['date'] = bitcoin['Date'].copy()\n",
        "bitcoin.index = bitcoin['date']\n",
        "del bitcoin['date']\n",
        "# combine sentiment and bitcoin price data\n",
        "merged_data = bitcoin.merge(df_vader, on='Date')\n",
        "print(merged_data.shape)\n",
        "merged_data.head()\n"
      ],
      "metadata": {
        "id": "TEeHHI846cGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# new df with formatted variables and iDid a bit of manipulation here too\n",
        "merged_data_ = pd.DataFrame()\n",
        "merged_data_['price'] = merged_data['Close']\n",
        "merged_data_['volatility'] = (merged_data['High'] - merged_data['Low']) / merged_data['Close']#just the formula to get vola\n",
        "merged_data_['volume_price'] = merged_data['Volume']\n",
        "merged_data_['volume_number'] = merged_data['Volume'] / merged_data['Close']\n",
        "merged_data_['positive_comment'] = merged_data['positive_comment']\n",
        "merged_data_['neutral_comment'] = merged_data['neutral_comment']\n",
        "merged_data_['negative_comment'] = merged_data['negative_comment']\n",
        "merged_data_.index = merged_data['Date']\n",
        "merged_data_.tail()"
      ],
      "metadata": {
        "id": "TCfPbj_xSHbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Statistical Test assumption , that the data is stationary\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.tsa.stattools import kpss"
      ],
      "metadata": {
        "id": "iuz4R-yDSHdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(merged_data_.isnull().sum())#just checking null\n",
        "\n",
        "# descriptive statistics\n",
        "def descriptive_statistics(merged_data_, series):\n",
        "    stats = merged_data_[series].describe()    \n",
        "    print('\\nDescriptive Statistics for', '\\'' + series + '\\'', '\\n\\n', stats)\n",
        "\n",
        "# create data visuals function\n",
        "def get_graphics(merged_data_, series, xlabel, ylabel, title, grid = True):\n",
        "    plt.plot(pd.to_datetime(merged_data_.index), merged_data_[series])\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.title(title)\n",
        "    plt.grid(grid)\n",
        "    return plt.show()    "
      ],
      "metadata": {
        "id": "546UGrJVSHgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#How stationary test Assumption works\n",
        "#unit root = statistical properties of series are not constant with time.\n",
        "\n",
        "#In order to be stationary, series has to be constant with time. So if a series has a unit root, it is not stationary sha\n",
        "\n",
        "#strict stationary = mean, variance, covariance are not function of time\n",
        "#trend stationary = no root unit, but has a trend. if you remove the trend, it would be strict stationary\n",
        "#difference stationary = series can be made strict stationary by differencing\n",
        "\n",
        "#ADF Augmented Dickey Fuller Test (unit root test)\n",
        "#null hypothesis = series has a unit root (a = 1)\n",
        "#alt hypothesis = series has no unit root\n",
        "\n",
        "#accept null = t-score is greter than critical value (there is a unit root)\n",
        "#reject null = t-score is less than critical value (there is no unit root)\n",
        "\n",
        "#accpet null = bad (not stationary)\n",
        "#reject null = good (stationary)\n",
        "\n",
        "#adf can be interpreted as a difference stationary test"
      ],
      "metadata": {
        "id": "bSAUv2qdX3vL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ADF Augmented Dickey Fuller Test\n",
        "def adf_test(merged_data_, series):\n",
        "    results = adfuller(merged_data_[series])\n",
        "    output = pd.Series(results[0:4], index = ['t-score', 'p-value', '# of lags used', '# of observations'])\n",
        "    for key, value in results[4].items():\n",
        "        output['critical value (%s)' %key] = value\n",
        "    # if t-score < critical value at 5%, the data is stationary\n",
        "    # if t-score > critical value at 5%, the data is NOT stationary\n",
        "    if output[0] < output[5]:\n",
        "        print('\\nADF: The data', '\\'' + series + '\\'', 'is STATIONARY \\n\\n', output)\n",
        "    elif output[0] > output[5]:\n",
        "        print('\\nADF: The data', '\\'' + series + '\\'', 'is NOT STATIONARY \\n\\n', output)\n",
        "    else:\n",
        "        print('\\nADF: There is something wrong with', '\\'' + series + '\\'','\\n\\n', output)"
      ],
      "metadata": {
        "id": "ILF-TtIAX3xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#KPSS Kwiatkowski-Phillips-Schmidt-Shin Test (stationary test)\n",
        "#null hypothesis = that the timeseries has a stationary trend\n",
        "#alternative hypothesis = that the timeseries dataset has a unit root (series is not stationary)\n",
        "\n",
        "#accept null = t-score is less than critical value (series is stationary)\n",
        "#reject null = t-score is greater than the critical value (series is not stationary)\n",
        "\n",
        "#accpet null = good (stationary)\n",
        "#reject null = bad (not stationary)\n",
        "\n",
        "#kpss normally classifies a series as stationary on the absence of a unit root\n",
        "#\n",
        "#(both strict stationary and trend stationary will be classified as stationary)"
      ],
      "metadata": {
        "id": "3wlDK6etZfGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# KPSS Kwiatkowski-Phillips-Schmidt-Shin Test (stationary test)\n",
        "def kpss_test(merged_data_, series):\n",
        "    results = kpss(merged_data_[series], regression = 'ct')\n",
        "    output = pd.Series(results[0:3], index = ['t-score', 'p-value', '# lags used'])              \n",
        "    for key, value in results[3].items():\n",
        "        output['critical value (%s)' %key] = value\n",
        "    # if t-score < critical value at 5%, the data is stationary\n",
        "    # if t-score > critical value at 5%, the data is NOT stationary\n",
        "    if output[0] < output[4]:\n",
        "        print('\\nKPSS: The data', '\\'' + series + '\\'', 'is STATIONARY \\n\\n', output)\n",
        "    elif output[0] > output[4]:\n",
        "        print('\\nKPSS: The data', '\\'' + series + '\\'', 'is NOT STATIONARY \\n\\n', output)\n",
        "    else:\n",
        "        print('\\nKPSS: There is something wrong with', '\\'' + series + '\\'', '\\n\\n', output)"
      ],
      "metadata": {
        "id": "Y47mu6BkZfIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sometimes , adf and kpss statistical test can give you conflicting results. if so then say\n",
        "\n",
        "#[adf = stationary], [kpss = stationary] = series is stationary\n",
        "#[adf = stationary], [kpss = NOT stationary] = series is difference stationary. use differencing to make it stationary\n",
        "#[adf = NOT stationary], [kpss = stationary] = series is trend stationary. remove trend to make strict stationary\n",
        "#[adf = NOT STATIONARY], [kpss = NOT STATIONARY] = series is not stationary"
      ],
      "metadata": {
        "id": "bEWEdqovZfLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## everything combined"
      ],
      "metadata": {
        "id": "u_dHqsptfXi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# so all functions above in one\n",
        "def series_analysis(merged_data_, series, xlabel, ylabel, title, grid = True):\n",
        "    # descriptive stats\n",
        "    descriptive_statistics(merged_data_, series)\n",
        "    # graphics\n",
        "    get_graphics(merged_data_, series, xlabel, ylabel, title, grid = True)\n",
        "    # stationary tests\n",
        "    adf_test(merged_data_, series)\n",
        "    kpss_test(merged_data_, series)"
      ],
      "metadata": {
        "id": "tUT1_EBWZfO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a  new df for stationary data\n",
        "\n",
        "stationary = pd.DataFrame()\n",
        "stationary\n",
        "# ['price'] \n",
        "#plt.rcParams[\"figure.figsize\"] = (20,10)\n",
        "series_analysis(merged_data_, 'price', xlabel = 'year', ylabel = 'Bitcoin Price(USD)', title = 'merged_data_[\\'price\\']')"
      ],
      "metadata": {
        "id": "Iy7WclhhX31r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lets try one more to make it stationary by taking both the log and 1-day difference "
      ],
      "metadata": {
        "id": "-RBPhAgwf_eH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ['price'] = log & diff\n",
        "stationary['price'] = merged_data_['price'].apply(np.log).diff().dropna()\n",
        "# run tests to see if stationary\n",
        "series_analysis(stationary, 'price', xlabel = 'year', ylabel = 'Bitcoin Price(USD)', title = 'Log_Diff_Price')\n",
        "\n",
        "stationary.head()"
      ],
      "metadata": {
        "id": "FK0NSD63f_gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ['volatility']\n",
        "series_analysis(merged_data_, 'volatility', xlabel = 'year', ylabel = 'volatility (daily (high-low)/price)', title = 'merged_data_[\\'volatility\\']')"
      ],
      "metadata": {
        "id": "PJYKt8nvf_j1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ['volatility'] = diff\n",
        "stationary['volatility'] = merged_data_['volatility'].diff().dropna()\n",
        "# run tests to see if stationary\n",
        "series_analysis(stationary, 'volatility',  xlabel = 'year', ylabel = 'Volatility', title = 'Diff_Volatility')\n",
        "\n",
        "stationary.head()"
      ],
      "metadata": {
        "id": "fLDoO7Jginld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#volume_price\n",
        "\n",
        "# ['volume_price']\n",
        "series_analysis(merged_data_, 'volume_price', xlabel = 'year', ylabel = 'Volume_Price(USD)', title = 'merged_data_[\\'volume_price\\']')"
      ],
      "metadata": {
        "id": "fAfFhVd5inoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ['volume_price'] = log & diff\n",
        "stationary['volume_price'] =  merged_data_['volume_price'].apply(np.log).diff().dropna()\n",
        "# run tests to see if stationary\n",
        "series_analysis(stationary, 'volume_price',  xlabel = 'year', ylabel = 'volume_price', title = 'Log_Diff_Volume_Price')\n",
        "\n",
        "stationary.head()"
      ],
      "metadata": {
        "id": "tlHu_7oUinri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ['volume_number'] that is number of bitcoin people exchanged\n",
        "series_analysis(merged_data_, 'volume_number', xlabel = 'year', ylabel = 'Number of Bitocins exchanged', title = 'merged_data_[\\'volume_number\\']')"
      ],
      "metadata": {
        "id": "deG5edNwxRFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ['volume_number'] = diff\n",
        "stationary['volume_number'] = merged_data_['volume_number'].diff().dropna()\n",
        "# run tests to see if stationary\n",
        "series_analysis(stationary, 'volume_number',  xlabel = 'year', ylabel = 'volume_number', title = 'Diff_Volume_Number')\n",
        "\n",
        "stationary.head()"
      ],
      "metadata": {
        "id": "5ptyZEYyl_ty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ['positive_comment']\n",
        "series_analysis(merged_data_, 'positive_comment', xlabel = 'year', ylabel = 'Number of Positive Comments', title = 'merged_data_[\\'positive_comment\\']')"
      ],
      "metadata": {
        "id": "u1ZdGVtkl_wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ['positive_comment'] = log & diff \n",
        "stationary['positive_comment'] = merged_data_['positive_comment'].apply(np.log).diff().dropna()\n",
        "# run tests to see if stationary\n",
        "series_analysis(stationary, 'positive_comment', xlabel = 'year', ylabel = 'Number of Positive Comments', title = 'Log_Diff_Positive_Comment')\n",
        "\n",
        "stationary.head()"
      ],
      "metadata": {
        "id": "_B18Rsinl_zK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ['neutral_comment']\n",
        "series_analysis(merged_data_, 'neutral_comment', xlabel = 'year', ylabel = 'Number of Neutral Comments', title = 'merged_data_[\\'neutral_comment\\']')"
      ],
      "metadata": {
        "id": "HhiROHPBl_14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ['neutral_comment'] = log & diff\n",
        "stationary['neutral_comment'] = merged_data_['neutral_comment'].apply(np.log).diff().dropna()\n",
        "# run tests to see if stationary\n",
        "series_analysis(stationary, 'neutral_comment', xlabel = 'year', ylabel = 'Number of Neutral Comments', title = 'Log_Diff_Neutral_Comment')\n",
        "\n",
        "stationary.head()"
      ],
      "metadata": {
        "id": "Qcu6J9yBl_4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ['negative_comment']\n",
        "series_analysis(merged_data_, 'negative_comment', xlabel = 'year', ylabel = 'Number of Negative Comments', title = 'merged_data_[\\'negative_comment\\']')"
      ],
      "metadata": {
        "id": "BtaKIQqGl_60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ['negative_comment'] = log & diff \n",
        "stationary['negative_comment'] = merged_data_['negative_comment'].apply(np.log).diff().dropna()\n",
        "# run tests to see if stationary\n",
        "series_analysis(stationary, 'negative_comment', xlabel = 'year', ylabel = 'Number of Negative Comments', title = 'Log_Diff_Negative_Comment')\n",
        "\n",
        "stationary.head()"
      ],
      "metadata": {
        "id": "EMzLYggsl_9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now let's apply machine learning on both data we just transformed till this stage"
      ],
      "metadata": {
        "id": "s37S-XZtmAAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.api import VAR\n",
        "# create train data (80/20 split)\n",
        "train = stationary[ : int(0.8*(len(stationary)))].copy()\n",
        "print(train.shape)\n",
        "\n",
        "# create VAR model on train data\n",
        "model = VAR(endog = train)"
      ],
      "metadata": {
        "id": "zHXLLpXvn0x5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see the different lag order suggestions to pick your information criterion\n",
        "print(model.select_order(trend = 'c'))\n",
        "\n",
        "# fit model to train data\n",
        "# model_fit is a VARResultsWrapper object\n",
        "model_fit = model.fit(ic = 'aic', trend = 'c')\n",
        "# number of lags\n",
        "num_lag = model_fit.k_ar\n",
        "num_lag"
      ],
      "metadata": {
        "id": "xjBfASvXn0-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_fit summary\n",
        "model_fit.summary()"
      ],
      "metadata": {
        "id": "f2INgPKzoZKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example one day forecast into the future using train data\n",
        "## IMPORTANT = data has to be in ndarray format. use '.values' on y argument ##\n",
        "model_fit.forecast(y = train.values, steps = 1)"
      ],
      "metadata": {
        "id": "l2T_rpnipEfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating var functions\n",
        "df = merged_data_.copy()"
      ],
      "metadata": {
        "id": "F1WvSNK0pEhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create function so that we can iterate model_fit.forecast() over test data\n",
        "# VAR also has the method 'predict' but it doesn't work so we have to do this the hard way though\n",
        "\n",
        "def predict(data, fitted_model, lag_order, predict_steps):\n",
        "    # empty list for our predictions\n",
        "    prediction = []\n",
        "  \n",
        "    # for loop to iterate fitted_model over data\n",
        "    for i in range(lag_order, len(data)):\n",
        "        # window of lagged data that the model uses to predict next observation\n",
        "        window = data.iloc[i - lag_order : i].copy()\n",
        "        # results of fitted_model being applied to window\n",
        "        results = fitted_model.forecast(y = window.values, steps = predict_steps)\n",
        "        # append results to prediction list\n",
        "        prediction.append(results)\n",
        "        \n",
        "    # convert prediction (which is a list of numpy arrays) to a dataframe\n",
        "    df = np.vstack(prediction)\n",
        "    df = pd.DataFrame(df)\n",
        "    # df column names from data\n",
        "    df.columns = list(data.columns)\n",
        "    # df index from data\n",
        "    df.index = data.iloc[len(data) - len(prediction) :].index\n",
        "    \n",
        "    # return df\n",
        "    return df"
      ],
      "metadata": {
        "id": "2xtnc-WDpElG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create error functions\n",
        "\n",
        "# root mean squared error\n",
        "def rmse(predicted, actual):\n",
        "    # formula for rmse\n",
        "    residual = predicted - actual\n",
        "    residual_sq = residual ** 2\n",
        "    mean_sq = np.mean(residual_sq)\n",
        "    rmse_value = np.sqrt(mean_sq)\n",
        "    # return rmse_value\n",
        "    return rmse_value\n",
        "\n",
        "# mean absolute error\n",
        "def mae(predicted, actual):\n",
        "    # formula for mae\n",
        "    absolute_residual = np.absolute(predicted - actual)\n",
        "    mae_value = np.mean(absolute_residual)\n",
        "    # return mae_value\n",
        "    return mae_value"
      ],
      "metadata": {
        "id": "RLZGd9HDoZM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function that returns lineplot of predicted vs actual data w/ rmse and mae, and histograph of the residual\n",
        "\n",
        "def model_graphs(predicted, actual, title = str):\n",
        "    # RMSE\n",
        "    rmse_value = rmse(predicted = predicted, actual = actual)\n",
        "    # MAE\n",
        "    mae_value = mae(predicted = predicted, actual = actual)\n",
        "    # start_year (for putting in text box)\n",
        "    start_year = predicted.iloc[ : 1].index.copy()\n",
        "    # text box in line plot\n",
        "    text_str = 'RMSE = ' + str(rmse_value) + '\\n MAE = ' + str(mae_value)\n",
        "    # line plot\n",
        "    plt.figure(1)\n",
        "    plt.plot(actual, color = 'blue', linewidth = 2, label = 'actual')\n",
        "    plt.plot(predicted, color = 'red', linewidth = 1, label = 'predicted')\n",
        "    plt.legend()\n",
        "    plt.title(title + ' Actual vs Predicted')\n",
        "    plt.text(x = start_year, y = 0.2, s = text_str)\n",
        "    # residual & hist\n",
        "    plt.figure(2)\n",
        "    residual = actual - predicted\n",
        "    plt.hist(residual, bins = 200)\n",
        "    plt.title('Distribution of ' + title + ' residual')\n",
        "    plt.axvline(residual.mean(), color = 'k', linestyle = 'dashed', linewidth = 1)\n",
        "    # show graphics\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Y_zMN-6NoZQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function classifies price movement each day as either going up(if positive) or down(negative)\n",
        "def category(x):\n",
        "    if x >= 0:\n",
        "        return 'up'\n",
        "    elif x < 0:\n",
        "        return 'down'\n",
        "\n",
        "# function that returns confusion matrix of model with metrics\n",
        "def confusion_matrix(predicted, actual, title = str):\n",
        "    df = pd.DataFrame()\n",
        "    df['predicted'] = predicted.apply(category)\n",
        "    df['actual'] = actual.apply(category)\n",
        "    # code\n",
        "    df.loc[(df['predicted'] == 'up') & (df['actual'] == 'up'), 'code'] = 'true_positive'\n",
        "    df.loc[(df['predicted'] == 'up') & (df['actual'] == 'down'), 'code'] = 'false_positive'\n",
        "    df.loc[(df['predicted'] == 'down') & (df['actual'] == 'down'), 'code'] = 'true_negative'\n",
        "    df.loc[(df['predicted'] == 'down') & (df['actual'] == 'up'), 'code'] = 'false_negative'\n",
        "    # confusion dictionary\n",
        "    z = dict(df['code'].value_counts())\n",
        "    # confusion metrics\n",
        "    accuracy = (z['true_positive'] + z['true_negative']) / (z['true_positive'] + z['true_negative'] + z['false_positive'] + z['false_negative'])\n",
        "    true_positive_rate = z['true_positive'] / (z['true_positive'] + z['false_negative'])\n",
        "    false_positive_rate = z['false_positive'] / (z['false_positive'] + z['true_negative'])\n",
        "    true_negative_rate = z['true_negative'] / (z['true_negative'] + z['false_positive'])\n",
        "    false_negative_rate = z['false_negative'] / (z['false_negative'] + z['true_positive'])\n",
        "    # print metrics\n",
        "    print('\\nMetrics for [{0}]\\nAccuracy:{1:6.3f} \\nTP Rate:{2:7.3f} \\nFP Rate:{3:7.3f}\\nTN Rate:{4:7.3f} \\nFN Rate:{5:7.3f}'.format(str(title), accuracy, true_positive_rate, false_positive_rate, true_negative_rate, false_negative_rate))\n",
        "    # print confusion matrix graph\n",
        "    print('\\n'+\n",
        "      '            [{title}] Confusion Matrix\\n'.format(title = str(title))+\n",
        "      '\\n'+\n",
        "      '           |-------------|-------------|\\n'+\n",
        "      '  n= {0}  | Predicted:  | Predicted:  |\\n'.format(z['true_positive']+z['false_positive']+z['true_negative']+z['false_negative'])+\n",
        "      '           |    Down     |    Up       |\\n'+\n",
        "      '|----------|-------------|-------------|------------|\\n'+\n",
        "      '| Actual:  |             |             |            |\\n'+\n",
        "      '|  Down    |  tn: {0}    |  fp: {1}    |    {2}     |\\n'.format(z['true_negative'], z['false_positive'], z['true_negative']+z['false_positive'])+\n",
        "      '|----------|-------------|-------------|------------|\\n'+\n",
        "      '| Actual:  |             |             |            |\\n'+\n",
        "      '|   UP     |  fn: {0}    |  tp: {1}    |    {2}    |\\n'.format(z['false_negative'], z['true_positive'] ,z['false_negative']+z['true_positive'])+\n",
        "      '|----------|-------------|-------------|------------|\\n'+\n",
        "      '           |             |             |\\n'+\n",
        "      '           |      {0}    |      {1}   |\\n'.format(z['true_negative']+z['false_negative'], z['false_positive']+z['true_positive'])+\n",
        "      '           |-------------|-------------|\\n')\n",
        "    # return df\n",
        "    return df"
      ],
      "metadata": {
        "id": "8ZyiwvwDoZat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now lets try look at var model results\n",
        "\n",
        "# train results\n",
        "train_predicted = model_fit.fittedvalues.copy()\n",
        "train_actual = train.iloc[num_lag : len(train)]\n",
        "\n",
        "# graphs\n",
        "model_graphs(predicted = train_predicted['price'], actual = train_actual['price'], title = 'Training')"
      ],
      "metadata": {
        "id": "1Cl-llTeoZeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# confusion matrix performance metrics\n",
        "train_confusion = confusion_matrix(predicted = train_predicted['price'], actual = train_actual['price'], title = 'Train')"
      ],
      "metadata": {
        "id": "tmiPuxoUoZm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing "
      ],
      "metadata": {
        "id": "eIjEYT2tn1A7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_L8aXn8tn1Df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PvTMuatDn1G8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0zmm0fWGn1X2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}